{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# M115 - Image Analysis and Processing, Assignment 2 (Notebook 1)\n\n---","metadata":{}},{"cell_type":"markdown","source":"In this assignment, an intelligent system was developed to detect pneumonia in chest X-ray images, utilising a dataset available at [Kaggle Chest X-Ray Images (Pneumonia) dataset](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia). The task involved the creation of two sorts of algorithms; classical Machine Learning and Deep Learning ones. This notebook covers the Classical ML aspect, and all the preprocessing thereof.\n\n\nThis coursework is submitted as part of the requirements for the Image Analysis and Processing (M115) course during the spring semester of 2023, in the DSIT's Master degree programme at the National and Kapodistrian University of Athens. The author of this project is\n\n- Michael Darmanis (SID: 7115152200004).\n\n\n\nParts of the code presented in the lecture have been used, and when foreign code (or parts of it) is invoked, it is explicitly mentioned.\n\nThe notebook was executed in Kaggle, so bear that in mind in case any issues arise while rerunning parts of the code locally (or in Google Colab, for that matter).\n\nWhatever the case may be, it is essential to **run all the necessary libraries (following four code cells) beforehand**, otherwise the instance of matplotlib will not function properly. It is also advised that all cells are run in of appearnce because many exercise are dependent (variable or otherwise) from previously calculated results.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport cv2\nfrom PIL import Image\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n\nfrom sklearn.metrics import f1_score, balanced_accuracy_score, precision_score, matthews_corrcoef, recall_score, make_scorer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom yellowbrick.model_selection import learning_curve\n\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n\nfrom sklearn.utils import shuffle as shf\nimport pickle\nimport os\nimport glob as gb\n\nimport cv2\nimport skimage\nfrom skimage import feature, filters\nfrom tqdm import tqdm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_random_images():\n    path_random_normal = random.choice(train_normal)\n    path_random_pneumonia = random.choice(train_pneumonia)\n    \n    fig = plt.figure(figsize=(10, 10))\n    \n    ax1 = plt.subplot(1, 2, 1)\n    ax1.imshow(Image.open(path_random_normal).convert(\"LA\"))\n    ax1.set_title(\"Normal X-ray\")\n    \n    ax2 = plt.subplot(1, 2, 2)\n    ax2.imshow(Image.open(path_random_pneumonia).convert(\"LA\"))\n    ax2.set_title(\"Pneumonia X-ray\")\n\n    \ndef print_metrics(y_pred, y_train, yt_pred, y_test):\n    print('Train data metrics:')\n    print('Balanced accuracy score: ', balanced_accuracy_score(y_train, y_pred))\n    print('F1 score: ', f1_score(y_train, y_pred))\n    print('Precison: ', precision_score(y_train, y_pred))\n    print('Recall: ', recall_score(y_train, y_pred))\n    print('MCC',  matthews_corrcoef(y_train, y_pred))\n    print()\n    print('Test data metrics:')\n    print('Balanced accuracy score: ', balanced_accuracy_score(y_test, yt_pred))\n    print('F1 score: ', f1_score(y_test, yt_pred))\n    print('Precison: ', precision_score(y_test, yt_pred))\n    print('Recall: ', recall_score(y_test, yt_pred))\n    print('MCC',  matthews_corrcoef(y_test, yt_pred))\n\n\n#function to plot the confusion matrix for each model\ndef plot_confusion_matrix(predictions, y_test, title):\n    labels = ['Normal', 'Pnuemonia']\n    \n    cm = confusion_matrix(y_test,predictions)\n    cm = pd.DataFrame(cm , index = ['0','1'] , columns = ['0','1'])\n    \n    plt.figure(figsize = (10, 10))\n    plt.title(title)\n    sns.heatmap(cm, linecolor = 'black' , linewidth = 1,\n                annot = True, fmt='', xticklabels = labels,\n                yticklabels = labels)\n    plt.show()\n    \n    \ndef evaluate_classifiers(X_test, y_test):\n    # Create a dictionary to store your classifiers\n    classifiers = {'Logistic Regression': log_reg, 'Decision Tree': dtc,\n                   'Random Forest': rfc, 'SVM': svm}\n\n    # Initialize an empty dictionary to store the scores for each classifier\n    scores = {'Classifier': [], 'F1': [], 'Balanced Accuracy': [], 'Precision': [],\n              'MCC': [], 'Recall': []}\n\n    # Iterate over the classifiers\n    for clf_name, clf in classifiers.items():\n        y_pred = clf.predict(X_test)\n\n        # Calculate scores\n        f1 = f1_score(y_test, y_pred, average='weighted') \n        balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n        precision = precision_score(y_test, y_pred, average='weighted')\n        mcc = matthews_corrcoef(y_test, y_pred)\n        recall = recall_score(y_test, y_pred, average='weighted')\n\n        # Add scores to the scores dictionary\n        scores['Classifier'].append(clf_name)\n        scores['F1'].append(f1)\n        scores['Balanced Accuracy'].append(balanced_accuracy)\n        scores['Precision'].append(precision)\n        scores['MCC'].append(mcc)\n        scores['Recall'].append(recall)\n\n    return pd.DataFrame(scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data exploration","metadata":{}},{"cell_type":"code","source":"train_normal = gb.glob(\"../input/chest-xray-pneumonia/chest_xray/train/NORMAL/*\")\ntrain_pneumonia = gb.glob(\"../input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA/*\")\n\ntest_normal = gb.glob(\"../input/chest-xray-pneumonia/chest_xray/test/NORMAL/*\")\ntest_pneumonia = gb.glob(\"../input/chest-xray-pneumonia/chest_xray/test/PNEUMONIA/*\")\n\nval_normal = gb.glob(\"../input/chest-xray-pneumonia/chest_xray/val/NORMAL/*\")\nval_pneumonia = gb.glob(\"../input/chest-xray-pneumonia/chest_xray/val/PNEUMONIA/*\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train normal:', len(train_normal), '   ', 'Train pneumonia:', len(train_pneumonia))\nprint('Test normal:', len(test_normal), '     ', 'Test pneumonia:', len(test_pneumonia))\nprint('Validation normal:', len(val_normal), ' ', 'Validation pneumonia:', len(val_pneumonia))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First of all, the validation size is insignificant. It will therefore be concatenated with the training data and, should validation data be required, the training data will be randomly split at a 9-1 analogy.\n\nThere also exists an imbalance of classes. To mitigate this issue, the `class_weight` parameter will be employed with the value `balanced`, for all ML models used. The `class_weight` argument modifies the loss function during training by assigning a higher penalty for misclassifying the minority class.\n\nIn the face of class imbalance, the implementation of metrics that evaluate performance across all classes, also becomes crucial. Therefore the following metrics will be employed throughout:\n- Balanced accuracy\n- F1 score\n- Precision\n- Recall\n- Matthews correlation coefficient\n\nBalanced Accuracy, which computes the average recall for each class, provides an effective solution for imbalanced datasets due to its indifference towards the majority class. The F1 score, representing the harmonic mean of precision and recall, is also valuable, especially when the positive class bears more significance and a balance between Precision (representing the accuracy of positive predictions) and Recall (indicating the detection rate of all positive instances) is sought.\n\nAdditionally, Precision and Recall are key metrics that permit model fine-tuning in specific directions. Precision, the ratio of true positive predictions to the total predicted positives, becomes vital when the reduction of false positives is the goal. Conversely, Recall, the ratio of true positive predictions to all actual positive instances, is pivotal when the maximization of positive instance detection is the objective. The Matthews Correlation Coefficient (MCC), an all-encompassing measure for binary classifications, takes into account both true and false positives and negatives, thereby offering a balanced metric, particularly useful when the classes are of vastly different sizes.\n\nWithin the framework of the currect project, thse metrics will provide an appropriate evaluation of the models' performance; permitting necessary adjustments for managing class imbalance.","metadata":{}},{"cell_type":"markdown","source":"Next, random images from both train and test sets are checked.","metadata":{}},{"cell_type":"code","source":"show_random_images()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_random_images()\ndel train_normal, test_normal, val_normal\ndel train_pneumonia, test_pneumonia, val_pneumonia","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Upon inspection, the images seems to vary in both size and rotation (slightly); this will be born to mind when preprocessing the data and when fitting the ML models.","metadata":{}},{"cell_type":"markdown","source":"### On-the-spot attempt","metadata":{}},{"cell_type":"markdown","source":"For a first, on-the-spot, attempt; a basic preprocessing will be perfomed. More particularly, the images will be reduced to 300 by 300 pixels and a normalisation & Principal Component Analysis will be applied.\n\n<div style=\"text-align:center\">\n<img src=\"https://i.imgur.com/gW78vT9.png\" alt=\"workflow\" width=\"500\" height=\"600\"/>\n</div>\n\nFour classifier models will be trained: Linear Regression, Random Forest, Decision Tree, and a Support Vector Machine. The model yielding the best output in terms of the chosen metrics will then be parameter-tuned using a grid search.","metadata":{}},{"cell_type":"code","source":"code = {'NORMAL':0 ,'PNEUMONIA':1}\n#function to return the class of the images from its number, so the function would return 'Normal' if given 0, and 'PNEUMONIA' if given 1.\ndef getcode(n) : \n    for x , y in code.items() : \n        if n == y : \n            return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the directories that contain the train and validation images set\npaths = ['../input/chest-xray-pneumonia/chest_xray/train/', \n         '../input/chest-xray-pneumonia/chest_xray/val/']\n\nX_train = []\ny_train = []\n\nfor trainpath in paths:\n    for folder in  os.listdir(trainpath) : \n        files = gb.glob(pathname= str( trainpath + folder + '/*.jpeg'))\n        for file in files: \n            image = cv2.imread(file)\n            #resize images to 300 x 300 pixels\n            image_array = cv2.resize(image , (300, 300))\n            X_train.append(list(image_array))\n            y_train.append(code[folder])\n\nX_train = np.asarray(X_train)\nX_train = X_train.astype(np.float32)\nnp.save('X_train', X_train)\ndel X_train\n\ny_train = np.asarray(y_train)\ny_train = y_train.astype(np.float32)\nnp.save('y_train', y_train)\ndel y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the directory that contain the test images set\ntestpath='../input/chest-xray-pneumonia/chest_xray/test/'\n\nX_test = []\ny_test = []\nfor folder in  os.listdir(testpath) : \n    files = gb.glob(pathname= str( testpath + folder + '/*.jpeg'))\n    for file in files: \n        image = cv2.imread(file)\n        #resize images to 300 x 300 pixels\n        image_array = cv2.resize(image , (300, 300))\n        X_test.append(list(image_array))\n        y_test.append(code[folder])\n\nX_test = np.asarray(X_test)\nX_test = X_test.astype(np.float32)\nnp.save('X_test',X_test)\ndel X_test\n\ny_test = np.asarray(y_test)\ny_test = y_test.astype(np.float32)\nnp.save('y_test',y_test)\ndel y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_train, X_test contain the images as numpy arrays, while y_train, y_test contain the class of each image \nloaded_X_train = np.load('./X_train.npy')\nloaded_X_test = np.load('./X_test.npy')\nloaded_y_train = np.load('./y_train.npy')\nloaded_y_test = np.load('./y_test.npy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#flatten the images into a 2d array, for model training and testing\nX_train = loaded_X_train.reshape([-1, np.product((300, 300, 3))])\nX_test = loaded_X_test.reshape([-1, np.product((300, 300, 3))])\ndel loaded_X_train, loaded_X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = loaded_y_train\ny_test = loaded_y_test\ndel loaded_y_train, loaded_y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shuffle train and test data sets in a consistent way\nX_train, y_train = shf(X_train, y_train, random_state=15)\nX_test, y_test = shf(X_test, y_test, random_state=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scaling\nsc = StandardScaler(copy=False)\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#PCA    \npca = PCA(.95)\npca.fit(X_train)\nX_train = pca.transform(X_train)\nX_test = pca.transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#printing the variance of each component from PCA\nprint('Number of components after PCA: ' + str(pca.n_components_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#making an instance for each algorithm\nlog_reg  = LogisticRegression(class_weight='balanced')\ndtc  = DecisionTreeClassifier(class_weight='balanced')\nrfc = RandomForestClassifier(class_weight='balanced')\nsvm = SVC(class_weight='balanced')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fitting each model using X_train and y_train\nlog_reg.fit(X_train, y_train)\ndtc.fit(X_train, y_train)\nrfc.fit(X_train, y_train)\nsvm.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the dictionary to a pandas DataFrame\ndf_scores = evaluate_classifiers(X_test, y_test)\nprint(df_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mcc_scorer = make_scorer(matthews_corrcoef)\n\n# Defining parameter range\nparam_grid_svm = {'C': [0.1, 1, 10],  \n              'gamma': [1, 0.1, 0.01, 0.001],\n              'kernel': ['rbf', 'linear', 'poly']} \n\ngrid_svm = GridSearchCV(svm, param_grid_svm, refit = True, verbose = 3, scoring=mcc_scorer)\n  \n# fitting the model for grid search on the training data\ngrid_svm.fit(X_train, y_train)\n\n# Inspect the best parameters found by GridSearchCV\nprint('Best parameters for SVM:', grid_svm.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best model\nbest_svm = grid_svm.best_estimator_\n\n# Calculate and print the metrics\nprint_metrics(best_svm.predict(X_train), y_train, best_svm.predict(X_test), y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(best_svm.predict(X_test), y_test, 'Optimised SVM')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_train, y_train, X_test, y_test, pca, sc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Extraction","metadata":{}},{"cell_type":"markdown","source":"As an alternative, and more nuanced approach, traditional image processing methods, with the aim of extracting features for classification tasks, will be applied. This is accomplished by using a subset of available image processing techniques. Here's a brief overview of the steps involved and their importance:\n\n1. **Equalisation**: Image equalisation enhances the contrast of the lungs and accentuates the presence of opacity. This will most likely improve contrast which facilitates subsequent feature extraction.\n\n2. **Image Sharpening**: High pass filtering is used for image sharpening because it reveals more detail compared to the unmask method. Sharpening the image allows for the extraction of more precise and detailed features.\n\n3. **Otsu Thresholding**: Otsu thresholding technique provides smoother edges and better lung segment isolation, which is why it's used in this pipeline.\n\n4. **Edge Detection**: For edge detection, the Sobel filter is chosen as it extracts edges more effectively than the Canny filter in similar [projects](https://doi.org/10.1038/s41551-021-00787-w).\n\n5. **Moment Calculation**: Once the lung segment is identified, the center of the moment is calculated as a feature for prediction. As many X-ray images do not have the same dimensions, this feature can pose a problem however.\n\n6. **Rotation and Scale Invariance**: Some images may present subjects in slightly rotated positions or different sizes. Therefore, the center of the moment is needed to be invariant to rotation and scale. Hu moments are chosen for this purpose, and to make comparison easier, the moments are logged. The third moment, which depends on the other moments, and the seventh moment, which distinguishes mirror images, are dropped, as no flipped images were observed in the dataset.\n\nThe adopted pipeline is seen in the following picture:\n\n<div style=\"text-align:center\">\n<img src=\"https://i.imgur.com/5d3W9p5.png\" alt=\"workflow\" width=\"500\" height=\"600\"/>\n</div>\n\nThe selected features for building a classifier for pneumonia detection, therefore, are:\n* Mean and Standard Deviation of unenhanced image\n* Area of opacity\n* Perimeter of visible lung regions\n* Irregularity index\n* Equivalent diameter\n* Hu moments (5 out of 7)\n\nThe same classifiers will be used as previously, and the best vanilla model will be parameter-tuned.","metadata":{}},{"cell_type":"code","source":"def area(img):\n    # binarized image as input\n    return np.count_nonzero(img)\n\ndef perimeter(img):\n    # edges of the image as input\n    return np.count_nonzero(img)\n\ndef irregularity(area, perimeter):\n    # area and perimeter of the image as input, also called compactness\n    I = (4 * np.pi * area) / (perimeter ** 2)\n    return I\n\ndef equiv_diam(area):\n    # area of image as input\n    ed = np.sqrt((4 * area) / np.pi)\n    return ed\n\ndef get_hu_moments(contour):\n    # hu moments except 3rd and 7th (5 values)\n    M = cv2.moments(contour)\n    hu = cv2.HuMoments(M).ravel().tolist()\n    del hu[2], hu[-1]\n    log_hu = [-np.sign(a)*np.log10(np.abs(a)) for a in hu]\n    return log_hu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_features(img):\n    \"\"\"\n    The function  carries out the steps mentioned above on an input image.\n    It begins with basic statistical calculations (mean and standard deviation),\n    applies image processing techniques such as histogram equalisation,\n    sharpening, thresholding, and edge detection.\n    Then, it finds contours and selects the one with the most points.\n    The function then calculates various features such as area, perimeter,\n    irregularity, equivalent diameter, and Hu moments from the selected contour.\n    It then returns these calculated features for the given input image.\n    \"\"\"\n    mean = img.mean()\n    std_dev = img.std()\n    \n    # Histogram equalisation\n    equalized = cv2.equalizeHist(img)\n    \n    # Sharpening\n    hpf_kernel = np.full((3, 3), -1)\n    hpf_kernel[1,1] = 9\n    sharpened = cv2.filter2D(equalized, -1, hpf_kernel)\n    \n    # Thresholding\n    ret, binarized = cv2.threshold(cv2.GaussianBlur(sharpened, \n                                        (7, 7), 0), 0, 255, \n                                   cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    # Edge detection\n    edges = skimage.filters.sobel(binarized)\n    \n    # Moments from contours\n    contours, hier = cv2.findContours((edges * 255).astype('uint8'), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n    select_contour = sorted(contours, key=lambda x: x.shape[0], reverse=True)[0]\n    \n    # Feature extraction\n    ar = area(binarized)\n    per = perimeter(edges)\n    irreg = irregularity(ar, per)\n    eq_diam = equiv_diam(ar)\n    hu = get_hu_moments(select_contour)\n    \n    return [mean, std_dev, ar, per, irreg, eq_diam, *hu]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the directories that contain the train and validation images set\npaths = ['../input/chest-xray-pneumonia/chest_xray/train/', \n         '../input/chest-xray-pneumonia/chest_xray/val/']\n\nX_train = []\n\nfor trainpath in paths:\n    for folder in  os.listdir(trainpath) : \n        files = gb.glob(pathname= str( trainpath + folder + '/*.jpeg'))\n        for file in files: \n            image = cv2.imread(file)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            \n            # feature extraction\n            features = extract_features(image)\n            X_train.append(features)\n\nX_train = np.asarray(X_train)\nX_train = X_train.astype(np.float32)\nnp.save('X_train_fe', X_train)\ndel X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the directory that contain the test images set\ntestpath='../input/chest-xray-pneumonia/chest_xray/test/'\n\nX_test = []\nfor folder in  os.listdir(testpath) : \n    files = gb.glob(pathname= str( testpath + folder + '/*.jpeg'))\n    for file in files: \n        image = cv2.imread(file)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            \n        # feature extraction\n        features = extract_features(image)\n        X_test.append(features)\n\nX_test = np.asarray(X_test)\nX_test = X_test.astype(np.float32)\nnp.save('X_test_fe',X_test)\ndel X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.load('./X_train_fe.npy')\nX_test = np.load('./X_test_fe.npy')\ny_train = np.load('./y_train.npy')\ny_test = np.load('./y_test.npy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scaling\nsc = StandardScaler(copy=False)\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fitting each model using X_train and y_train\nlog_reg.fit(X_train, y_train)\ndtc.fit(X_train, y_train)\nrfc.fit(X_train, y_train)\nsvm.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the dictionary to a pandas DataFrame\ndf_scores = evaluate_classifiers(X_test, y_test)\nprint(df_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mcc_scorer = make_scorer(matthews_corrcoef)\n\n# Defining parameter range\nparam_grid_svm = {'C': [0.1, 1, 10],  \n              'gamma': [1, 0.1, 0.01, 0.001],\n              'kernel': ['rbf', 'linear', 'poly']} \n\ngrid_svm = GridSearchCV(svm, param_grid_svm, refit = True,\n                        verbose = 3, scoring=mcc_scorer)\n  \n# Fitting the model for grid search on the training data\ngrid_svm.fit(X_train, y_train)\n\n# Inspect the best parameters found by GridSearchCV\nprint('Best parameters for SVM:', grid_svm.best_params_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best model\nbest_svm = grid_svm.best_estimator_\n\n# Calculate and print the metrics\nprint_metrics(best_svm.predict(X_train), y_train, best_svm.predict(X_test), y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confustion_matrix(best_svm.predict(X_test), y_test, 'Optimised SVM')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Best ML Intelligent System","metadata":{}},{"cell_type":"code","source":"feats = list(extract_features(img))\n\npred = gb.predict([feats])\n\nif pred == 1:\n    print('Patient is infected with pneumonia')\nelse:\n    print('Patient is normal')","metadata":{},"execution_count":null,"outputs":[]}]}