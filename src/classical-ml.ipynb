{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# M115 - Image Analysis and Processing, Assignment 2 (Notebook 1)\n\n---","metadata":{}},{"cell_type":"markdown","source":"In this assignment, an intelligent system was developed to detect pneumonia in chest X-ray images, utilising a dataset available at [Kaggle Chest X-Ray Images (Pneumonia) dataset](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia). The task involved the creation of two sorts of algorithms; classical Machine Learning and Deep Learning ones. This notebook covers the Classical ML aspect, and all the preprocessing thereof.\n\n\nThis coursework is submitted as part of the requirements for the Image Analysis and Processing (M115) course during the spring semester of 2023, in the DSIT's Master degree programme at the National and Kapodistrian University of Athens. The author of this project is\n\n- Michael Darmanis (SID: 7115152200004).\n\nParts of code presented in this [notebook](https://www.kaggle.com/code/hosen42/pneumonia-detection-using-traditional-ml) have been used.\n\nThe notebook was executed in Kaggle, so bear that in mind in case any issues arise while rerunning parts of the code locally (or in Google Colab, for that matter).\n\nWhatever the case may be, it is essential to **run all the necessary libraries (following four code cells) beforehand**, otherwise the instance of matplotlib will not function properly. It is also advised that all cells are run in of appearnce because many exercise are dependent (variable or otherwise) from previously calculated results or created instances.","metadata":{}},{"cell_type":"code","source":"# Dependencies for rendering text of matplotlib in LaTeX\n!sudo apt update -y\n!sudo apt install -y cm-super dvipng texlive-latex-extra texlive-latex-recommended","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport cv2\nfrom PIL import Image\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n\nfrom sklearn.metrics import f1_score, balanced_accuracy_score, precision_score, matthews_corrcoef, recall_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\n\nfrom sklearn.metrics import confusion_matrix\n\n\nfrom sklearn.utils import shuffle as shf\nimport pickle\nimport os\nimport glob as gb\n\n\nimport cv2\nimport skimage\nfrom skimage import feature, filters\nfrom tqdm import tqdm\n\n# Consider aesthetics and consistency\nplt.rcParams.update({\n        \"text.usetex\": True,\n        \"font.size\": 15,\n        'mathtext.default': 'regular',\n        'axes.titlesize': 16,\n        \"axes.labelsize\": 16,\n        \"legend.fontsize\": 15,\n        \"xtick.labelsize\": 15,\n        \"ytick.labelsize\": 15,\n        'figure.titlesize': 16,\n        'figure.figsize': (12, 7),\n        'text.latex.preamble': r'\\usepackage{amsmath,amssymb}',\n        \"font.family\": \"serif\",\n        \"font.serif\": \"computer modern roman\",\n        })","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_random_images():\n    path_random_normal = random.choice(train_normal)\n    path_random_pneumonia = random.choice(train_pneumonia)\n    \n    fig = plt.figure(figsize=(10, 10))\n    \n    ax1 = plt.subplot(1, 2, 1)\n    ax1.imshow(Image.open(path_random_normal).convert(\"LA\"))\n    ax1.set_title(\"Normal X-ray\")\n    \n    ax2 = plt.subplot(1, 2, 2)\n    ax2.imshow(Image.open(path_random_pneumonia).convert(\"LA\"))\n    ax2.set_title(\"Pneumonia X-ray\")\n\n    \ndef print_metrics(y_pred, y_train, yt_pred, y_test):\n    print('Train data metrics:')\n    print('Balanced accuracy score: ', balanced_accuracy_score(y_train, y_pred))\n    print('F1 score: ', f1_score(y_train, y_pred))\n    print('Precison: ', precision_score(y_train, y_pred))\n    print('Recall: ', recall_score(y_train, y_pred))\n    print('MCC',  matthews_corrcoef(y_train, y_pred))\n    print()\n    print('Test data metrics:')\n    print('Balanced accuracy score: ', balanced_accuracy_score(y_test, yt_pred))\n    print('F1 score: ', f1_score(y_test, yt_pred))\n    print('Precison: ', precision_score(y_test, yt_pred))\n    print('Recall: ', recall_score(y_test, yt_pred))\n    print('MCC',  matthews_corrcoef(y_test, yt_pred))\n\n\ndef plot_confusion_matrix(predictions, y_test, title):\n    labels = ['Normal', 'Pnuemonia']\n    \n    cm = confusion_matrix(y_test,predictions)\n    cm = pd.DataFrame(cm , index = labels , columns = labels)\n    \n    plt.figure()\n    sns.heatmap(cm, cmap=\"YlGnBu\", linecolor = 'black' , linewidth = 1,\n                annot = True, fmt='', xticklabels = labels,\n                yticklabels = labels)\n    \n    plt.title(title, fontsize = 20)\n    plt.xlabel('Predicted', fontsize = 15)\n    plt.ylabel('Actual', fontsize = 15)\n    \n    plt.savefig(f'/kaggle/working/{title.replace(\" \", \"_\")}_cm.pdf',\n                format='pdf', bbox_inches='tight', dpi=300, transparent=True)\n    plt.show()\n    \n    \ndef evaluate_classifiers(X_test, y_test):\n    # Create a dictionary to store your classifiers\n    classifiers = {'Logistic Regression': lg, 'Decision Tree': dtc,\n                   'Random Forest': rfc, 'SVM': svm}\n\n    # Initialize an empty dictionary to store the scores for each classifier\n    scores = {'Classifier': [], 'F1': [], 'Balanced Accuracy': [], 'Precision': [],\n              'MCC': [], 'Recall': []}\n\n    # Iterate over the classifiers\n    for clf_name, clf in classifiers.items():\n        y_pred = clf.predict(X_test)\n\n        # Calculate scores\n        f1 = f1_score(y_test, y_pred, average='weighted') \n        balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n        precision = precision_score(y_test, y_pred, average='weighted')\n        mcc = matthews_corrcoef(y_test, y_pred)\n        recall = recall_score(y_test, y_pred, average='weighted')\n\n        # Add scores to the scores dictionary\n        scores['Classifier'].append(clf_name)\n        scores['F1'].append(f1)\n        scores['Balanced Accuracy'].append(balanced_accuracy)\n        scores['Precision'].append(precision)\n        scores['MCC'].append(mcc)\n        scores['Recall'].append(recall)\n\n    return pd.DataFrame(scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data exploration\n\nVisualise and understand the dataset's properties, including data distribution, class imbalance, and data quality.","metadata":{}},{"cell_type":"code","source":"train_normal = gb.glob(\"../input/chest-xray-pneumonia/chest_xray/train/NORMAL/*\")\ntrain_pneumonia = gb.glob(\"../input/chest-xray-pneumonia/chest_xray/train/PNEUMONIA/*\")\n\ntest_normal = gb.glob(\"../input/chest-xray-pneumonia/chest_xray/test/NORMAL/*\")\ntest_pneumonia = gb.glob(\"../input/chest-xray-pneumonia/chest_xray/test/PNEUMONIA/*\")\n\nval_normal = gb.glob(\"../input/chest-xray-pneumonia/chest_xray/val/NORMAL/*\")\nval_pneumonia = gb.glob(\"../input/chest-xray-pneumonia/chest_xray/val/PNEUMONIA/*\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Train normal:', len(train_normal), '   ', 'Train pneumonia:', len(train_pneumonia))\nprint('Test normal:', len(test_normal), '     ', 'Test pneumonia:', len(test_pneumonia))\nprint('Validation normal:', len(val_normal), ' ', 'Validation pneumonia:', len(val_pneumonia))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First of all, the validation size is insignificant. It will therefore be concatenated with the training data and, should validation data be required, the training data will be randomly split at a 9-1 analogy.\n\nThere also exists an imbalance of classes. To mitigate this issue, the `class_weight` parameter will be employed with the value `balanced`, for all ML models used. The `class_weight` argument modifies the loss function during training by assigning a higher penalty for misclassifying the minority class.\n\nIn the face of class imbalance, the implementation of metrics that evaluate performance across all classes, also becomes crucial. Therefore the following metrics will be employed throughout:\n- Balanced accuracy\n- F1 score\n- Precision\n- Recall\n- Matthews correlation coefficient\n\nBalanced Accuracy, which computes the average recall for each class, provides an effective solution for imbalanced datasets due to its indifference towards the majority class. The F1 score, representing the harmonic mean of precision and recall, is also valuable, especially when the positive class bears more significance and a balance between Precision (representing the accuracy of positive predictions) and Recall (indicating the detection rate of all positive instances) is sought.\n\nAdditionally, Precision and Recall are key metrics that permit model fine-tuning in specific directions. Precision, the ratio of true positive predictions to the total predicted positives, becomes vital when the reduction of false positives is the goal. Conversely, Recall, the ratio of true positive predictions to all actual positive instances, is pivotal when the maximization of positive instance detection is the objective. The Matthews Correlation Coefficient (MCC), an all-encompassing measure for binary classifications, takes into account both true and false positives and negatives, thereby offering a balanced metric, particularly useful when the classes are of vastly different sizes.\n\nWithin the framework of the currect project, these metrics will provide an satisfactory evaluation of the models' performance; permitting necessary adjustments for managing class imbalance.","metadata":{}},{"cell_type":"markdown","source":"Next, random images from both train and test sets are checked.","metadata":{}},{"cell_type":"code","source":"show_random_images()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_random_images()\ndel train_normal, test_normal, val_normal\ndel train_pneumonia, test_pneumonia, val_pneumonia","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Upon inspection, the images seems to vary in both size and rotation (slightly); this will be born to mind when preprocessing the data and when fitting the ML models.","metadata":{}},{"cell_type":"markdown","source":"## On-the-spot attempt","metadata":{}},{"cell_type":"markdown","source":"For a first, on-the-spot, attempt; a basic preprocessing will be perfomed. More particularly, the images will be reduced to 300 by 300 pixels and a normalisation & Principal Component Analysis will be applied.\n\n<div style=\"text-align:center\">\n<img src=\"https://i.imgur.com/gW78vT9.png\" alt=\"workflow\" width=\"500\" height=\"600\"/>\n</div>\n\nFour classifier models will be trained: Linear Regression, Random Forest, Decision Tree, and a Support Vector Machine. The model yielding the best output in terms of the chosen metrics will then be parameter-tuned using a grid search.","metadata":{}},{"cell_type":"code","source":"# Assign class of the X-rays based on folder name\ncode = {'NORMAL':0 ,'PNEUMONIA':1}\ndef getcode(n) : \n    for x , y in code.items() : \n        if n == y : \n            return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Directories that contain the train and validation images set\npaths = ['../input/chest-xray-pneumonia/chest_xray/train/', \n         '../input/chest-xray-pneumonia/chest_xray/val/']\n\nX_train = []\ny_train = []\n\nfor trainpath in paths:\n    for folder in  os.listdir(trainpath) : \n        files = gb.glob(pathname= str( trainpath + folder + '/*.jpeg'))\n        for file in files: \n            image = cv2.imread(file)\n            # Resize images to 300 x 300 pixels\n            image_array = cv2.resize(image , (300, 300))\n            X_train.append(list(image_array))\n            y_train.append(code[folder])\n\nX_train = np.asarray(X_train)\nX_train = X_train.astype(np.float32)\nnp.save('X_train', X_train)\ndel X_train\n\ny_train = np.asarray(y_train)\ny_train = y_train.astype(np.float32)\nnp.save('y_train', y_train)\ndel y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Directory that contains the test images set\ntestpath='../input/chest-xray-pneumonia/chest_xray/test/'\n\nX_test = []\ny_test = []\nfor folder in  os.listdir(testpath) : \n    files = gb.glob(pathname= str( testpath + folder + '/*.jpeg'))\n    for file in files: \n        image = cv2.imread(file)\n        # Resize images to 300 x 300 pixels\n        image_array = cv2.resize(image , (300, 300))\n        X_test.append(list(image_array))\n        y_test.append(code[folder])\n\nX_test = np.asarray(X_test)\nX_test = X_test.astype(np.float32)\nnp.save('X_test',X_test)\ndel X_test\n\ny_test = np.asarray(y_test)\ny_test = y_test.astype(np.float32)\nnp.save('y_test',y_test)\ndel y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load datasets\nX_train = np.load('./X_train.npy')\nX_test = np.load('./X_test.npy')\ny_train = np.load('./y_train.npy')\ny_test = np.load('./y_test.npy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Flatten the images into a 2d array, for model training and testing\nX_train = X_train.reshape([-1, np.product((300, 300, 3))])\nX_test = X_test.reshape([-1, np.product((300, 300, 3))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shuffle train and test datasets in a consistent way\nX_train, y_train = shf(X_train, y_train, random_state=15)\nX_test, y_test = shf(X_test, y_test, random_state=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalise train and test datasets\nsc = StandardScaler(copy=False)\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Principal component analysis with 95% retained variance \npca = PCA(.95)\npca.fit(X_train)\nX_train = pca.transform(X_train)\nX_test = pca.transform(X_test)\n\nprint('Number of components after PCA: ' + str(pca.n_components_))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make an instance for each and every classification algorithm\n# notice the class_weight option set to balanced\nlg  = LogisticRegression(class_weight='balanced')\ndtc = DecisionTreeClassifier(class_weight='balanced')\nrfc = RandomForestClassifier(class_weight='balanced')\nsvm = SVC(class_weight='balanced')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting each vanilla model\nlg.fit(X_train, y_train)\ndtc.fit(X_train, y_train)\nrfc.fit(X_train, y_train)\nsvm.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show evaluation metrics for all algorithms\ndf_scores = evaluate_classifiers(X_test, y_test)\nprint(df_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on all these metrics, it would appear that the SVM model offers the best performance on the unbalanced X-ray dataset. Therefore, a brute-force parameter tuning will be performed based on the F1 score.","metadata":{}},{"cell_type":"code","source":"# Defining parameter range\nparam_grid_svm = {'C': [0.1, 1, 10],  \n              'gamma': [1, 0.1, 0.01, 0.001],\n              'kernel': ['rbf', 'linear', 'poly']} \n\ngrid_svm = GridSearchCV(svm, param_grid_svm,\n                        refit = True, verbose = 3,\n                        scoring='f1')\n  \n# Fitting the model for grid search on the training data\ngrid_svm.fit(X_train, y_train)\n\n# Inspect the best parameters found by GridSearchCV\nprint('Best parameters for SVM:', grid_svm.best_params_)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best model\nbest_svm = grid_svm.best_estimator_\n\n# Calculate and print the metrics\nprint_metrics(best_svm.predict(X_train), y_train,\n              best_svm.predict(X_test), y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(best_svm.predict(X_test), y_test, 'Optimised SVM Model')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del X_train, y_train, X_test, y_test, pca, sc","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature-extraction attempt","metadata":{}},{"cell_type":"markdown","source":"As an alternative, and more nuanced approach, traditional image processing methods will be applied. The aim now will be that of extracting features for the classification task.\n\nPneumonia can readily be observed by lung oppacities, therefore, segmenting and focusing only on the lung part can be of importance (see this [notebook](https://www.kaggle.com/code/mikedarm/what-are-lung-opacities)).\n\nThis will be accomplished by using numerous image processing techniques. Here's a brief overview of the steps involved and their importance:\n\n1. **Equalisation**: Image equalisation enhances the contrast of the lungs and accentuates the presence of opacity. This will most likely improve contrast which facilitates subsequent feature extraction.\n\n2. **Image Sharpening**: High pass filtering is used for image sharpening because it reveals more detail compared to the unmask method. Sharpening the image allows for the extraction of more precise and detailed features.\n\n3. **Otsu Thresholding**: Otsu thresholding technique provides smoother edges and better lung segment isolation, which is why it's used in this pipeline.\n\n4. **Edge Detection**: For edge detection, the Sobel filter is chosen as it extracts edges more effectively than the Canny filter in similar [projects](https://doi.org/10.1038/s41551-021-00787-w).\n\n5. **Moment Calculation**: Once the lung segment is identified, the center of the moment is calculated as a feature for prediction. As many X-ray images do not have the same dimensions, this feature can pose a problem however.\n\n6. **Rotation and Scale Invariance**: Some images may present subjects in slightly rotated positions or different sizes. Therefore, the center of the moment is needed to be invariant to rotation and scale. Hu moments are chosen for this purpose ([Digital Image Processing](https://dl.acm.org/doi/book/10.5555/22881)), and to make comparison easier, the moments are logged. The third moment, which depends on the other moments, and the seventh moment, which distinguishes mirror images, are dropped, as no flipped images were observed in the dataset.\n\nThe adopted pipeline is seen in the following picture:\n\n<div style=\"text-align:center\">\n<img src=\"https://i.imgur.com/5d3W9p5.png\" alt=\"workflow\" width=\"500\" height=\"600\"/>\n</div>\n\nThe selected features for building a classifier for pneumonia detection, therefore, are:\n* Mean and Standard Deviation of unenhanced image\n* Area of opacity\n* Perimeter of visible lung regions\n* Irregularity index\n* Equivalent diameter\n* Hu moments (5 out of 7)\n\nThe same classifiers will be used as previously, and the best vanilla model will be parameter-tuned.","metadata":{}},{"cell_type":"code","source":"def area(img):\n    # Binarized image as input\n    return np.count_nonzero(img)\n\ndef perimeter(img):\n    # Edges of the image as input\n    return np.count_nonzero(img)\n\ndef irregularity(area, perimeter):\n    # Area and perimeter of the image as input (i.e. compactness)\n    I = (4 * np.pi * area) / (perimeter ** 2)\n    return I\n\ndef equiv_diam(area):\n    # Area of image as input\n    ed = np.sqrt((4 * area) / np.pi)\n    return ed\n\ndef get_hu_moments(contour):\n    # Hu moments except 3rd and 7th (5 values)\n    M = cv2.moments(contour)\n    hu = cv2.HuMoments(M).ravel().tolist()\n    del hu[2], hu[-1]\n    log_hu = [-np.sign(a)*np.log10(np.abs(a)) for a in hu]\n    return log_hu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_features(img):\n    \"\"\"\n    The function  carries out the steps mentioned above on an input image.\n    It begins with basic statistical calculations (mean and standard deviation),\n    applies image processing techniques such as histogram equalisation,\n    sharpening, thresholding, and edge detection.\n    Then, it finds contours and selects the one with the most points.\n    The function then calculates various features such as area, perimeter,\n    irregularity, equivalent diameter, and Hu moments from the selected contour.\n    It then returns these calculated features for the given input image.\n    \"\"\"\n    mean = img.mean()\n    std_dev = img.std()\n    \n    # Histogram equalisation\n    equalized = cv2.equalizeHist(img)\n    \n    # Sharpening\n    hpf_kernel = np.full((3, 3), -1)\n    hpf_kernel[1,1] = 9\n    sharpened = cv2.filter2D(equalized, -1, hpf_kernel)\n    \n    # Thresholding\n    ret, binarized = cv2.threshold(cv2.GaussianBlur(sharpened, \n                                        (7, 7), 0), 0, 255, \n                                   cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    # Edge detection\n    edges = skimage.filters.sobel(binarized)\n    \n    # Moments from contours\n    contours, hier = cv2.findContours((edges * 255).astype('uint8'), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n    select_contour = sorted(contours, key=lambda x: x.shape[0], reverse=True)[0]\n    \n    # Feature extraction\n    ar = area(binarized)\n    per = perimeter(edges)\n    irreg = irregularity(ar, per)\n    eq_diam = equiv_diam(ar)\n    hu = get_hu_moments(select_contour)\n    \n    return [mean, std_dev, ar, per, irreg, eq_diam, *hu]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Directories that contain the train and validation images set\n# (class labels are available from previous part)\npaths = ['../input/chest-xray-pneumonia/chest_xray/train/', \n         '../input/chest-xray-pneumonia/chest_xray/val/']\n\nX_train = []\n\nfor trainpath in paths:\n    for folder in  os.listdir(trainpath) : \n        files = gb.glob(pathname= str( trainpath + folder + '/*.jpeg'))\n        for file in files: \n            image = cv2.imread(file)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            \n            # Feature extraction\n            features = extract_features(image)\n            X_train.append(features)\n\nX_train = np.asarray(X_train)\nX_train = X_train.astype(np.float32)\nnp.save('X_train_fe', X_train)\ndel X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Directory that contain the test images set\n# (class labels are available from previous part)\ntestpath='../input/chest-xray-pneumonia/chest_xray/test/'\n\nX_test = []\nfor folder in  os.listdir(testpath) : \n    files = gb.glob(pathname= str( testpath + folder + '/*.jpeg'))\n    for file in files: \n        image = cv2.imread(file)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            \n        # Feature extraction\n        features = extract_features(image)\n        X_test.append(features)\n\nX_test = np.asarray(X_test)\nX_test = X_test.astype(np.float32)\nnp.save('X_test_fe',X_test)\ndel X_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load datasets\nX_train = np.load('./X_train_fe.npy')\nX_test = np.load('./X_test_fe.npy')\ny_train = np.load('./y_train.npy')\ny_test = np.load('./y_test.npy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Shuffle train and test datasets in a consistent way\nX_train, y_train = shf(X_train, y_train, random_state=15)\nX_test, y_test = shf(X_test, y_test, random_state=15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalise datasets\nsc = StandardScaler(copy=False)\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting each vanilla model\nlg.fit(X_train, y_train)\ndtc.fit(X_train, y_train)\nrfc.fit(X_train, y_train)\nsvm.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Show evaluation metrics for all algorithms\ndf_scores = evaluate_classifiers(X_test, y_test)\nprint(df_scores)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Considering the above metrics as a whole, Logistic Regression is the best model. However, the difference between Logistic Regression and SVM is quite small. Also the SVM's slightly higher precision might make it a good alternative, especially if avoiding false positives is a priority.\n\nFor the time being, the Logistic Regresion will be parameter-tuned.","metadata":{}},{"cell_type":"code","source":"# Defining parameter range\nparam_grid_lg = {\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n    'penalty': ['l1', 'l2'],\n    'max_iter': list(range(100, 800, 100)),\n    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n}\n\ngrid_lg = GridSearchCV(lg, param_grid=param_grid_lg, refit = True,\n                       verbose = 3, cv=5, scoring='f1')\n  \n# Fitting the model for grid search on the training data\ngrid_lg.fit(X_train, y_train)\n\n# Inspect the best parameters found by GridSearchCV\nprint('Best parameters for SVM:', grid_lg.best_params_)","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the best model\nbest_lg = grid_lg.best_estimator_\n\n# Calculate and print the metrics\nprint_metrics(best_lg.predict(X_train), y_train, best_lg.predict(X_test), y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(best_lg.predict(X_test), y_test, 'Optimised Logistic Regression Model')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is clear that using this pipeline, the best model (logistic regression) performs better than the one from the on-the-spot approach (SVM). The logistic regression may hit more false-possitives, however, it has greatly reduced the true-negatives (by 46).\n\nThe ideal model, therefore, for the chosen intelligent system will be the Logistic Regression using the proposed image preprocessing pipeline.","metadata":{}},{"cell_type":"markdown","source":"## Best ML Intelligent System","metadata":{}},{"cell_type":"markdown","source":"Download an X-ray image from [medschool.co](https://medschool.co/) and test the best classic Ml model.","metadata":{}},{"cell_type":"code","source":"!wget https://medschool.co/images/detail/cxr/consolidation-rml.jpg","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image = cv2.imread('/kaggle/working/consolidation-rml.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nfeatures = extract_features(image)\n\npred = best_lg.predict([features])\n\nif pred == 1:\n    print('Patient is infected with pneumonia')\nelse:\n    print('Patient is normal')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}