{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# M115 - Image Analysis and Processing, Assignment 2 (Notebook 2)\n\n---","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import random_split\nfrom torch.utils.data import WeightedRandomSampler\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom tqdm import tqdm\nimport time\nimport random\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pprint\n\n# Consider aesthetics and consistency\nplt.rcParams.update({\n        \"text.usetex\": True,\n        \"font.size\": 15,\n        'mathtext.default': 'regular',\n        'axes.titlesize': 16,\n        \"axes.labelsize\": 16,\n        \"legend.fontsize\": 15,\n        \"xtick.labelsize\": 15,\n        \"ytick.labelsize\": 15,\n        'figure.titlesize': 16,\n        'figure.figsize': (12, 7),\n        'text.latex.preamble': r'\\usepackage{amsmath,amssymb}',\n        \"font.family\": \"serif\",\n        \"font.serif\": \"computer modern roman\",\n        })","metadata":{"execution":{"iopub.status.busy":"2023-07-08T22:45:11.480571Z","iopub.execute_input":"2023-07-08T22:45:11.480951Z","iopub.status.idle":"2023-07-08T22:45:15.106118Z","shell.execute_reply.started":"2023-07-08T22:45:11.480922Z","shell.execute_reply":"2023-07-08T22:45:15.105128Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Starting off, check the available resources.","metadata":{}},{"cell_type":"code","source":"# Check if GPU is available and set the device accordingly\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\n\n# Get number of CPU cores\nnum_cpu_cores = os.cpu_count()\nprint(f\"Number of CPU cores: {num_cpu_cores}\")\n\n# Get GPU name\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    print(f\"GPU name: {gpu_name}\")\nelse:\n    print(\"No GPU available\")","metadata":{"execution":{"iopub.status.busy":"2023-07-08T22:45:15.108185Z","iopub.execute_input":"2023-07-08T22:45:15.108794Z","iopub.status.idle":"2023-07-08T22:45:15.157262Z","shell.execute_reply.started":"2023-07-08T22:45:15.108763Z","shell.execute_reply":"2023-07-08T22:45:15.156292Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Device: cuda\nNumber of CPU cores: 2\nGPU name: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Kaggle's Tesla P100 will be used for training and evaluating the neural model. Also 2 workers (2 CPUs) will be employed for the loading and batching of the data.","metadata":{}},{"cell_type":"markdown","source":"## Balance dataset","metadata":{}},{"cell_type":"markdown","source":"As was observed in Notebook 1, there is a class imbalance, with more pneumonia images than normal images. This can lead to a biased model that may not perform well on the under-represented class. To address this issue, the dataset will be balanced by applying oversampling, undersampling, or a combination of both to make the number of samples in each class equal. This can potentially help the CNN's performance on the under-represented class.","metadata":{}},{"cell_type":"code","source":"def create_weighted_sampler(h, dataset):\n    targets = [label for _, label in dataset]\n    class_counts = np.bincount(targets)\n    class_weights = 1.0 / class_counts\n    weights = [class_weights[label] for label in targets]\n    sampler = WeightedRandomSampler(weights, len(weights))\n    return sampler","metadata":{"execution":{"iopub.status.busy":"2023-07-08T22:45:15.158773Z","iopub.execute_input":"2023-07-08T22:45:15.159125Z","iopub.status.idle":"2023-07-08T22:45:15.167095Z","shell.execute_reply.started":"2023-07-08T22:45:15.159093Z","shell.execute_reply":"2023-07-08T22:45:15.165577Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{}},{"cell_type":"markdown","source":"As opposed to Notebook 1, the preprocessing here will be significantly different given than a CNN will be used.\n\nThe images' center will now be cropped, as opposed to resizing them, which could potentially distort their aspect ratio. The original proportions of the image are preserved by this alteration, aiding the CNN in better learning from the dataset and comprehending the inherent characteristics of the images.\n\nData augmentation techniques will also be leveraged to enhance the diversity of the training dataset. New training examples are generated through subtle alterations to the original images, such as rotation, flipping, and translation. Overfitting is effectively prevented by this strategy, enabling the CNN to become more resilient and proficient in generalising unseen data.\n\nTo correctly implement these techniques, a separate transformation for the training data has been designed, ensuring the test set remains unaffected by these augmentations. The same cautious approach is taken with the validation set, maintaining its authenticity by ensuring it does not undergo any data augmentation.\n\nNormalisation will still be performed on the whole dataset of course.\n\nHere's a brief overview of the aforementioned pipeline:\n\n<div style=\"text-align:center\">\n<img src=\"https://i.imgur.com/PHkp7ms.png\" alt=\"workflow3\" width=\"500\" height=\"600\"/>\n</div>\n\nWith the augmented volume of data, the CNN can be trained in more epochs without the risk of overfitting. This presents an opportunity to further refine the CNN's learning through additional training iterations, which should result in more accurate predictions.\n\nWith these enhancements and considerations in place, and enhanced ability of the CNN to perform better on unseen data while maintaining robustness against overfitting can reasonably be expected.","metadata":{}},{"cell_type":"code","source":"def prepare_data(h):\n    data_transforms = transforms.Compose([\n        transforms.Resize(size=(h[\"image_size\"], h[\"image_size\"])),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                            std=[0.229, 0.224, 0.225])\n    ])\n\n    data_transforms_train = transforms.Compose([\n        transforms.RandomRotation(20),                                               # Randomly rotate image\n        transforms.RandomHorizontalFlip(p=0.5),                                      # Randomly flip image\n        transforms.ColorJitter(brightness=0.1, contrast=0.1,\n                               saturation=0.1, hue=0.1),                             # Randomly change the b/c/h\n        transforms.RandomApply([transforms.RandomAffine(0, \n                               translate=(0.1, 0.1))], p=0.5),                       # Randomly apply affine transformations with translation\n        transforms.RandomApply([transforms.RandomPerspective(distortion_scale=0.2)],\n                               p=0.5),                                               # Randomly apply perspective transformations\n        transforms.Resize(size=(h[\"image_size\"], h[\"image_size\"])),                  # Resize data\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],                             # Normalise data\n                            std=[0.229, 0.224, 0.225])\n    ])\n\n\n    # Define the validation split percentage\n    val_split = 0.2\n\n    # Load the datasets\n    train_dataset = datasets.ImageFolder(\"/kaggle/input/chest-xray-pneumonia/chest_xray/train/\",\n                                         transform=data_transforms_train)\n    val_dataset = datasets.ImageFolder(\"/kaggle/input/chest-xray-pneumonia/chest_xray/train/\",\n                                       transform=data_transforms)    \n    test_dataset = datasets.ImageFolder(\"/kaggle/input/chest-xray-pneumonia/chest_xray/test/\",\n                                        transform=data_transforms)\n\n    # Create the data loaders for train, validation, and test sets\n    if (h[\"balance\"]):\n        sampler = create_weighted_sampler(h, train_dataset)\n        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=h[\"batch_size\"],\n                                                   sampler=sampler, num_workers=2)    \n    else:\n        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=h[\"batch_size\"],\n                                                   shuffle=True, num_workers=2)    \n    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=h[\"batch_size\"],\n                                             shuffle=True, num_workers=2)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=h[\"batch_size\"],\n                                              shuffle=True, num_workers=2)\n\n    return train_loader, val_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2023-07-08T22:45:15.170999Z","iopub.execute_input":"2023-07-08T22:45:15.171327Z","iopub.status.idle":"2023-07-08T22:45:15.184467Z","shell.execute_reply.started":"2023-07-08T22:45:15.171304Z","shell.execute_reply":"2023-07-08T22:45:15.183612Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Using a Convolutional Neural Network","metadata":{}},{"cell_type":"markdown","source":"As mentioned before, the model of choice will be a Convolutional Neural Network. CNNs are designed to work with image data and can capture local patterns more efficiently than fully connected layers. They can also handle larger input images, which can lead to better performance.\n\nThe custom CNN architecture will consist of multiple convolutional layers, followed by max-pooling layers, and finally a fully connected layer for classification. The image size will be set to 256x256, as CNNs can handle larger images better.\n\nHere's a brief overview of the CNN's architecture:\n\n<div style=\"text-align:center\">\n<img src=\"\" alt=\"workflow\" width=\"500\" height=\"600\"/>\n</div>","metadata":{}},{"cell_type":"code","source":"def create_model(h, device):\n    if (h[\"model\"]==\"cnn\"):\n        model = nn.Sequential(\n            nn.Conv2d(3, 16, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(16, 32, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Conv2d(32, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2, 2),\n            nn.Flatten(),\n            nn.Dropout(0.25),\n            nn.Linear(64 * (h[\"image_size\"] // 8) * (h[\"image_size\"] // 8), 512),\n            nn.ReLU(),\n            nn.Dropout(0.25),\n            nn.Linear(512, 2)\n        )\n        model = model.to(device)\n        return model","metadata":{"execution":{"iopub.status.busy":"2023-07-08T22:45:15.185884Z","iopub.execute_input":"2023-07-08T22:45:15.186885Z","iopub.status.idle":"2023-07-08T22:45:15.196930Z","shell.execute_reply.started":"2023-07-08T22:45:15.186853Z","shell.execute_reply":"2023-07-08T22:45:15.196025Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Training","metadata":{}},{"cell_type":"markdown","source":"Provided the augmentation of the dataset, the model will be trained for 10 epochs, using a 0.001 learning rate.","metadata":{}},{"cell_type":"code","source":"h = {\n    \"balance\": True,\n    \"num_epochs\": 10,\n    \"batch_size\": 256,\n    \"image_size\": 256,\n    \"lr\": 0.001,\n    \"model\": \"cnn\"\n}\n\n\ndef train_model(h, model, train_loader, val_loader, optimizer, criterion, device):\n    train_loss_history = []\n    val_loss_history = []\n\n    start_time = time.time()\n    num_epochs = h[\"num_epochs\"]\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n\n        progress_bar = tqdm(train_loader, desc=f\"Training epoch {epoch + 1}/{num_epochs}\", leave=False, unit=\"mini-batch\")\n        for inputs, labels in progress_bar:\n            inputs, labels = inputs.to(device), labels.to(device)      \n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n            progress_bar.set_postfix(loss=loss.item())\n\n\n        val_loss, _, _, _ = evaluate_model(h, model, val_loader, criterion, device)\n        \n        # Store the loss history\n        train_loss = running_loss / len(train_loader)\n        train_loss_history.append(train_loss)\n        val_loss_history.append(val_loss)\n\n        # Calculate elapsed time and remaining time\n        elapsed_time = time.time() - start_time\n        avg_time_per_epoch = elapsed_time / (epoch + 1)\n        remaining_epochs = num_epochs - (epoch + 1)\n        remaining_time = avg_time_per_epoch * remaining_epochs\n\n        # Convert remaining time to minutes and seconds\n        remaining_time_min, remaining_time_sec = divmod(remaining_time, 60)\n\n        print(f\"Epoch [{epoch + 1}/{num_epochs}]: Train Loss: {running_loss / len(train_loader):.4f}, Val Loss: {val_loss:.4f}, Remaining Time: {remaining_time_min:.0f}m {remaining_time_sec:.0f}s\")\n\n    return train_loss_history, val_loss_history\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-08T22:45:15.198407Z","iopub.execute_input":"2023-07-08T22:45:15.199015Z","iopub.status.idle":"2023-07-08T22:45:15.212212Z","shell.execute_reply.started":"2023-07-08T22:45:15.198980Z","shell.execute_reply":"2023-07-08T22:45:15.211165Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation","metadata":{}},{"cell_type":"markdown","source":"The F1 score will be used as a basic evaluation metric. It is useful for evaluating models when there is class imbalance, as it provides a more balanced measure of the model's performance. The F1 score ranges from 0 (worst) to 1 (best).","metadata":{}},{"cell_type":"code","source":"def evaluate_model(h, model, data_loader, criterion, device):\n    true_labels = []\n    predicted_labels = []\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    model.eval()\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)          \n\n            outputs = model(inputs)\n\n            loss = criterion(outputs, labels)\n            total_loss += loss.item()\n\n            _, predicted = torch.max(outputs.data, 1)\n            correct += (predicted == labels).sum().item()\n            total += labels.size(0)\n\n            true_labels.extend(labels.cpu().numpy())\n            predicted_labels.extend(predicted.cpu().numpy())\n\n    epoch_loss = total_loss / len(data_loader)\n    epoch_accuracy = correct / total\n\n    return epoch_loss, epoch_accuracy, true_labels, predicted_labels\n\n\ndef plot_metrics(h, train_loss_history, val_loss_history, test_loss, test_accuracy, true_labels, predicted_labels):\n    print(f\"Accuracy on the test set: {test_accuracy:.2%}\")\n\n    # Calculate precision, recall, and F1 score using the accumulated true labels and predictions\n    precision = precision_score(true_labels, predicted_labels)\n    recall = recall_score(true_labels, predicted_labels)\n    f1 = f1_score(true_labels, predicted_labels)\n    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}, F1 score: {f1:.2f}\")\n\n    # Calculate the confusion matrix using the accumulated true labels and predictions\n    cm = confusion_matrix(true_labels, predicted_labels)\n\n    # Visualize the confusion matrix\n    plt.figure()\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Normal\", \"Pneumonia\"])\n    plt.savefig(f'/kaggle/working/confMatrix.pdf',\n                format='pdf', bbox_inches='tight', dpi=300, transparent=True)\n    disp.plot()\n\n    # Plot the learning curves\n    plt.figure()\n    plt.plot(train_loss_history, label='Train Loss')\n    plt.plot(val_loss_history, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss history')\n    plt.legend()\n    plt.savefig(f'/kaggle/working/LossHistory.pdf',\n                format='pdf', bbox_inches='tight', dpi=300, transparent=True)\n    \n    plt.show()  ","metadata":{"execution":{"iopub.status.busy":"2023-07-08T22:45:15.213786Z","iopub.execute_input":"2023-07-08T22:45:15.214127Z","iopub.status.idle":"2023-07-08T22:45:15.227033Z","shell.execute_reply.started":"2023-07-08T22:45:15.214097Z","shell.execute_reply":"2023-07-08T22:45:15.225911Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Running the model and Statistical Significance","metadata":{}},{"cell_type":"markdown","source":"Since the training process is random, the training and testing will be repeated several times to measure the statistical significance of the results.","metadata":{}},{"cell_type":"code","source":"def check_solution(h, device, verbose):\n    train_loader, val_loader, test_loader = prepare_data(h)\n    model = create_model(h, device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=h[\"lr\"])\n    criterion = nn.CrossEntropyLoss()\n    train_loss_history, val_loss_history = train_model(h, model, train_loader,\n                                                       val_loader, optimizer, criterion, device)\n    test_loss, test_accuracy, true_labels, predicted_labels = evaluate_model(h, model, test_loader,\n                                                                             criterion, device)\n    if verbose:\n        plot_metrics(h, train_loss_history, val_loss_history, test_loss, test_accuracy,\n                     true_labels, predicted_labels)\n\n    f1 = f1_score(true_labels, predicted_labels)\n\n    return f1, test_accuracy\n\n\n# Print hyperparameters for records\nprint(\"Hyperparameters:\")\npprint.pprint(h)\n\nf1_array = np.array([])\naccuracy_array = np.array([])\nstart_time = time.time()\n\nrepeats = 5\nfor i in range(repeats):\n    print(f\"Running solution {i+1}/{repeats}\")\n    f1, accuracy = check_solution(h, device, verbose=(i==0))\n    print(f\"F1 = {f1:.2f}, accuracy = {accuracy:.2f} \")\n    f1_array = np.append(f1_array, f1)\n    accuracy_array = np.append(accuracy_array, accuracy) \n\n# Calculate elapsed time and remaining time\nrepeat_time = (time.time() - start_time) / repeats\nrepeat_time_min, repeat_time_sec = divmod(repeat_time, 60)\n\n# Printing final results\nprint(\"Results\")\nprint(f\"F1 List: {f1_array}\")\nprint(f\"Accuracy List: {accuracy_array}\")\nprint(f\"F1: {np.mean(f1_array):.1%} (+-{np.std(f1_array):.1%})\")\nprint(f\"Accuracy: {np.mean(accuracy_array):.1%} (+-{np.std(accuracy_array):.1%})\")\nprint(f\"Time of one solution: {repeat_time_min:.0f}m {repeat_time_sec:.0f}s\")\n\nprint(f\" | {np.mean(f1_array):.1%} (+-{np.std(f1_array):.1%}) | {np.mean(accuracy_array):.1%} (+-{np.std(accuracy_array):.1%}) | {repeat_time_min:.0f}m {repeat_time_sec:.0f}s\")","metadata":{"execution":{"iopub.status.busy":"2023-07-08T22:45:15.228420Z","iopub.execute_input":"2023-07-08T22:45:15.229344Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Hyperparameters:\n{'balance': True,\n 'batch_size': 256,\n 'image_size': 256,\n 'lr': 0.001,\n 'model': 'cnn',\n 'num_epochs': 10}\nRunning solution 1/5\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}}]}